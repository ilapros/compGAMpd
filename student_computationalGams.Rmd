---
title: "Some practical computational aspects of non-parametric (distributional) regression"
author: "Ilaria Prosdocimi - Ca' Foscari University of Venice"
date: "2023-03-06"
output:
  html_document:
            toc: true
            toc_float: true
            toc_collapsed: true
            toc_depth: 3
            code_folding: hide
---

## Preliminaries - data 

We'll make use of some dataset in the course of the session - mostly datasets avaiable in R packages, other than the data regarding the climate station at Hohenpeissenberg, in Germany, which is obtained from the DWD, the german climate office. 


```{r, include=TRUE}
climDat <- read.csv(file = "Hohenpeissenberg.csv", header = TRUE)
# climDat <- read.csv(file = "climateZug.csv", header = TRUE)
# with(climDat,plot(month,TMean))
# take yearly values 
yclimDat <- aggregate(climDat[,c("Precip","TMean","year")], by = list(climDat$year),FUN = mean, na.rm = TRUE)[,c("Precip","TMean","year")]
yclimDat$sYear <- (yclimDat$year-min(yclimDat$year))/diff(range(yclimDat$year))
yclimDat <- na.omit(yclimDat)
```

We will also use the following dataset: 

```{r}
data("airquality", package = "datasets")
data("mcycle", package = "MASS")
# this one might require you to install the package 
data("ethanol", package = "SemiPar")
# or load the data save in the repo manually
# write.table(ethanol, "ethanol.csv",sep=",",quote = FALSE)
```



## Refernces and inspirations

Key introduction books: 

Wood, Simon N. (2017) Generalized Additive Models: An Introduction with R, CRC, 2nd edition

Eilers, P.H.C. and Marx, B.D. (2021). Practical Smoothing, The Joys of P-splines. Cambridge University Press. ([book website](https://psplines.bitbucket.io))

Harezlak, J., Ruppert, D. and Wand, M.P., 2018. Semiparametric regression with R. New York: Springer. 

This latter book is the "follow-up" of 

Ruppert, D., Wand, M.P. and Carroll, R.J., 2003. Semiparametric regression. Cambridge university press.

Some slightly different philosophies to non-/semi-parametric regression presented in these books: they are all similar but there are some differences in the estimation approaches. 

The first original reference for Generalized Additive Models is 

Hastie, T. and Tibshirani, R., 1987. Generalized additive models: some applications. Journal of the American Statistical Association, 82(398), pp.371-386. 

which resulted in the book 

Hastie, T. and Tibshirani, R., 1990. Generalized additive models: some applications. CRC Press

but the Hastie and Tibshirani book/approach has been somewhat superseded (even if it is still valid, useful and relevant).

Key references on distributional regression/Structured Additive Regression (STAR) Models are: 

Stasinopoulos et al (2017), Flexible Regression and Smoothing: Using  GAMLSS  in R, CRC 

Fahrmeir L, Kneib T, Lang S, Marx B (2013). Regression â€“ Models, Methods and Applications. Springer-Verlag, Berlin.

An additional useful reference is: 

Thomas W. Yee (2015). Vector Generalized Linear and Additive Models: With an Implementation in R. New York, USA: Springer.

The journal Statistical Modelling published a [special issue](https://journals.sagepub.com/toc/smja/18/3-4) on the topic of flexible models for extending the classical (generalized) linear model framework with very practical tutorials on a number of extensions: most of these papers are very accessible and well written by leading experts on the topic. 

Today I will focus on some practical aspects of how these models are estimated in practice, which hopefully will help in understanding how they work.

## Why bother? 

The linear/polynomial model sometime is not flexible enough to model data which have particular features: 

```{r cars}
data(mcycle, package = "MASS")
# help(mcycle, package = "MASS")
plot(mcycle, pch = 16, col = "grey40")
nd <- data.frame(times = seq(2.4,57.6,by=0.1))
lines(nd$times, predict(lm(accel ~ times, data = mcycle), newdata = nd))
# doesn't cut it 
lines(nd$times, predict(lm(accel ~ poly(times,4), data = mcycle), newdata = nd), col = 4)
# not complex enough
lines(nd$times, predict(lm(accel ~ poly(times,6), data = mcycle), newdata = nd), col = 6)
rm(nd)
```

The fit is not particularity good and it is too wiggly in many parts of the domain. 

Furthermore polynomial regression is "global": 

```{r}
plot(mcycle, pch = 16, col = "orange2")
with(mcycle,lines(times, predict(lm(accel ~ poly(times,6))), col = 2, lwd = 1.4))
# TODO:  what happens when you use only data such that times > 5 and use a polynomial fit?
```


Exercise 2 of Chapter 4 in Wood's book gives an illuminating example about the local/global issue and how some local splines can overcome the problem (code slightly modified). We generate some noisy smooth data, try to fit them with polynomial fits and compare the interpolation properties of the polynomial fit with that obtained from a (regression) cubic spline basis of the shape: 
$$b_1(x) = 1, \ b_2(x) = x, \ b_{j+2}(x) = |x - x^*_j|^3, \text{ for } j= 1, \ldots, q-2$$
where $q$ is the basis dimension and $x^*_j$ are the knots locations (we'll talk more about this later). We'll take $q = 11$ and equally spaced knots over the range of $x$ values:
 
```{r}
# generate some noisy smooth data 
set.seed(1)
n <- 40
x <-sort(runif(n)*10)^.5
y <-sort(runif(n))^.1
plot(x,y, pch = 16, col = "grey40")
# estimate polynomial models and then use the model fit to interpolate 
nd <- data.frame(x = seq(0,4,by=0.05))
lines(nd$x, predict(lm(y ~ poly(x,5)), newdata = nd), col = 2)
lines(nd$x, predict(lm(y ~ poly(x,10)), newdata = nd), col = 4) # ups 
# define some cubic splines over knots 
cubSpline <- function(x, knots=NULL){ 
  X <- cbind(rep(1,length(x)), x)
  for(j in seq_along(knots)) X <- cbind(X, (abs(x-knots[j]))^3)
  X
} 
# define the knots 
xknots <-  seq(min(x),max(x), length.out = 9) # equally spaced 
lines(nd$x, cubSpline(nd$x, knots =  xknots) %*% coef(lm(y ~ cubSpline(x, knots =  xknots)-1)), col = "orange2")
legend("bottomright",bty = "n", col = c(2,4,"orange2"), legend = c("poly(5)","poly(10)","cubSpline"), lwd = 1.3)
rm(x,y,n,nd,xknots)
```

The fit obtained with the spline is flexible enough to capture the smooth relationship and does not have issues when interpolating in data sparse regions - let's see what would happen to the `mcycle` data: 


```{r}
# defined the knots 
xknots <-  seq(min(mcycle$times),max(mcycle$times), length.out = 9) # equally spaced 
plot(mcycle, pch = 16, col = rgb(0.93,0.60,0,0.6))
with(mcycle[mcycle$times <5,],points(accel ~ times, col = "grey40"))
# TODO: add cubic spline fit for the whole data and the case when you only use times > 5
```


Notice this is achieved using `lm`, we are simply fitting the usual linear model but using a custom-defined $\boldsymbol{X}$ matrix. So, how is the nice local behavior of the spline-based fit achieved? 

Let's look closely at the basis function: 

$$\text{Polynomial basis:} \ b_1(x) = 1, \ b_2(x) = x, \ b_3(x) = x^2 \ b_{j}(x) = x^{j-1}, \text{ for } j= 1, \ldots, d$$

$$\text{Cubic spline basis:} \ b_1(x) = 1, \ b_2(x) = x, \ b_{j+2}(x) = |x - x^*_j|^3, \text{ for } j= 1, \ldots, q-2$$


Let's plot the different columns of $\boldsymbol{X}$: 

```{r}
par(mfrow = c(1,2))
# polynomial matrix d = 6
poly6matrix <- with(mcycle, model.matrix(accel ~ poly(times,6)))
# cubic spline 
cSmatrix <- with(mcycle, model.matrix(accel ~ cubSpline(times,xknots)-1))
plot(mcycle$times,poly6matrix[,1], type = "l", ylim = range(poly6matrix))
for(j in 2:ncol(poly6matrix)) lines(mcycle$times,poly6matrix[,j],col = j)
plot(mcycle$times,cSmatrix[,1], type = "l", ylim = range(cSmatrix)) #, ylim = c(-0.6,15))
for(j in 2:ncol(cSmatrix)) lines(mcycle$times,cSmatrix[,j],col = j)
points(xknots,rep(-.1,9),pch = 4)
rm(xknots,cSmatrix,j,poly6matrix)
```



## Univariate Smoothers  

Let's start with a much simpler basis function than the cubic splines seen in action above and use a piecewise linear basis.

Let's build the tent function in R:

```{r}
## ToDO: 
# for a set of knots, build the jth basis 
tf <- function(x, knots, j){

}
par(mfrow=c(1,2))
# one tent function 
with(mcycle,plot(times, tf(times, seq(5,60,by=5),j=5), type = "l"))
#  the whole basis
with(mcycle,plot(times, tf(times, seq(5,60,by=5),j=1), type = "l"))
for(j in 2:9) with(mcycle,lines(times, tf(times, seq(5,60,by=5),j=j), col = j, type = "l"))
```

We can use this to construct a design matrix $\boldsymbol{X}$ to use for smoothing: 

```{r}
tf.X <- function(x,knots){
  nk <- length(knots); n <- length(x)
  X <- matrix(NA, nrow = n, ncol = nk)
  for(j in 1:nk) X[,j] <- tf(x, knots, j=j)
  X
}
Xtf_mcycle <- tf.X(mcycle$times, knots = seq(0,60,by=5))
fittf_mcycle <- lm(mcycle$accel ~ Xtf_mcycle-1)
plot(mcycle, pch = 16, col = "grey60")
valX <- tf.X(seq(3,60,by=0.5), knots = seq(0,60,by=5))
lines(seq(3,60,by=0.5), valX %*% coef(fittf_mcycle), col = 2)
points(seq(0,60,by=5), rep(-140,13),col=4,pch=4)
rm(Xtf_mcycle,fittf_mcycle,valX)
```


Let's see the effect of $\lambda$ in practice. First we need to construct $\boldsymbol{S} = \boldsymbol{D}^\top \boldsymbol{D}$, which we can create using `diff`: 


```{r}
## an example
diff(diag(5), differences = 2)
# the design matrix
Xtf_mcycle <- tf.X(mcycle$times, knots = seq(0,60,by=2))
# TODO: create S 
S <- matrix(NA)
## this is not the best way to compute this 
hatf_05 <- Xtf_mcycle %*% solve(crossprod(Xtf_mcycle)+.5*S) %*% t(Xtf_mcycle) %*% mcycle$accel
hatf_50 <- Xtf_mcycle %*% solve(crossprod(Xtf_mcycle)+50*S) %*% t(Xtf_mcycle) %*% mcycle$accel
hatf_500 <- Xtf_mcycle %*% solve(crossprod(Xtf_mcycle)+500*S) %*% t(Xtf_mcycle) %*% mcycle$accel
plot(mcycle, pch = 16, col = "grey60")
lines(mcycle$times, hatf_05, col = 4)
lines(mcycle$times, hatf_50, col = 2)
lines(mcycle$times, hatf_500, col = 6)
legend("topleft",col = c(2,4,6), legend = c(.5,50,500),title = "lambda value",lwd=1.4)
```

Let's look at the equivalent kernels: 

```{r}
# TODO compute and plot equivalent kernels
```


A more computationally stable way of obtaining the penalized least square fit is to exploit the fact that: 

$$||\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}||^2 + \lambda \boldsymbol{\beta}^\top \boldsymbol{S} \boldsymbol{\beta} = \left\Vert  \begin{bmatrix} \boldsymbol{y} \\ \boldsymbol{0} \end{bmatrix} - \begin{bmatrix} \boldsymbol{X} \\ \sqrt{\lambda} D \end{bmatrix} \boldsymbol{\beta} \right\Vert^2$$

so 

```{r}
X500augment <- rbind(Xtf_mcycle, sqrt(500) * diff(diag(ncol(Xtf_mcycle)), differences = 2))
yaugment <- c(mcycle$accel, rep(0, ncol(Xtf_mcycle)-2))
althatf_500 <- fitted(lm(yaugment ~ X500augment-1))
hatf_500 <- Xtf_mcycle %*% solve(crossprod(Xtf_mcycle)+500*S) %*% t(Xtf_mcycle) %*% mcycle$accel
head(althatf_500)
head(hatf_500[,1])
```




We can indeed write a generic function to construct penalized regression estimates: 

```{r}
# TODO write a function to perform penalized regression by augmenting the data
# expected output: an object of class lm
preg.fit <- function(y,x,knots,spar){

}
```


```{r,echo=FALSE,eval=FALSE}
rm(hatf_05,hatf_50,hatf_500,S,Xtf_mcycle,X500augment,yaugment,althatf_500,i,h05,h500,j)
```


### Smoothing as Regularization 


Let's see ridge regression in action: 

```{r}
set.seed(441)
n <- 6; p <- 10; # p > n
X <- scale(matrix(runif(n*p,0,1),ncol = p))
y <- rnorm(n,X %*% rnorm(p,0,2)); y <- y - mean(y)
f_orig <- lm(y~X-1); coef(f_orig)
f_ridge <- MASS::lm.ridge(y~X-1, lambda = 5); coef(f_ridge)
par(mfrow=c(1,2),pch=16)
plot(fitted(f_orig), y, xlab = "least square fit", ylab = "observed"); abline(0,1)
plot(X %*% coef(f_ridge), y, xlab = "ridge fit", ylab = "observed"); abline(0,1)
rm(n,X,y,p,f_orig,f_ridge)
```


### Smoothing parameter choice 


We now get to the tricky part of penalized regression splines (and regularized regression in general): choosing $\lambda$. There isn't a "best" way to choose $\lambda$, but there are some sensible metrics which can be used, such as cross-validation (CV), a generalized form of cross-validation (GCV), AIC and other likelihood and bayesian metrics which we don't discuss today. 

It can be shown that it is not actually necessary to hold-out the $i$th observation and iterate the estimation procedure $n$ times as we can derive that 
$$\mathcal{V}_o = \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{m}_{i})^2/(1-\boldsymbol{A}_{ii})^2$$
where $A$ is the hat matrix of the model (which in the penalized regression spline case depends on $\lambda$). 

```{r}
# TODO write the OCV function
# expected outcome: the OCV score (a scalar)
ocv <- function(lambda, y, x,knots){

}
# TODO: plot OCV as a function of lambda
# TODO plot the fitted smoother for the optimal lambda value 
```


A modification of the OCV is proposed, the so-called, Generalized Cross Validation (GCV), in which each element $\boldsymbol{A}_{ii}$ is replaced by $tr(\boldsymbol{A})/n$. We therefore define: 
$$\mathcal{V}_g = \frac{1}{n}  \sum_{i=1}^{n} \frac{(y_i - \hat{m}_{i})^2}{(1-tr(\boldsymbol{A})/n)^2}$$

Let's see it in action: 

```{r}
# TODO write the GCV function
# expected outcome: the GCV score (a scalar)
gcv <- function(lambda, y, x,knots){

}
# TODO: plot GCV as a function of lambda
# TODO plot the fitted smoother for the optimal lambda value 
gcv <- function(lambda, y, x,knots){

}
```




### Bayesian Linear Models 

A Bayesian models with a normal prior on the parameters: 

$$\boldsymbol{y} =  \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}$$ 

where $\boldsymbol{\varepsilon}$ is a vector of length $n$ of iid Gaussian error terms with mean 0 and constant known variance $\sigma^2$

We take a prior on the model parameters $\boldsymbol{\beta} \sim MVN(\boldsymbol{0}, \tau I)$ and wish to evaluate the posterior $p(\boldsymbol{\beta}|\boldsymbol{y})$: 

[...]

Thus $p(\boldsymbol{\beta}|\boldsymbol{y})$ is a $N(\boldsymbol{A}^{-1} \boldsymbol{b}, \boldsymbol{A}^{-1})$
and interestingly: 
$$E[\boldsymbol{\beta}|\boldsymbol{y}] = \frac{1}{\sigma^2} \left( \frac{1}{\sigma^2} \boldsymbol{X}^T \boldsymbol{X}  + \frac{1}{\tau^2} \boldsymbol{I} \right)^{-1} \boldsymbol{X}^T \boldsymbol{y} = \left(\boldsymbol{X}^T \boldsymbol{X}  + \frac{\sigma^2}{\tau^2} \boldsymbol{I} \right)^{-1} \boldsymbol{X}^T \boldsymbol{y}$$ 

Thus we see that if we assume a-priori that the coefficients are independent with a common variance we retrieve the ridge regression case. If we were to assume more complex structure in the prior distribution, eg $\boldsymbol{\beta} \sim MVN(\boldsymbol{0}, \tau \boldsymbol{S}^{-})$ we find that the MAP for $\beta$ corresponds to the $\hat{\beta}$ found for penalized regression. 

Let's assess the impact of $\tau$, the prior standard deviation of the $\beta$ coefficients, on the smoothness of the implied functions: 

```{r}
set.seed(454)
## let's generate a vector of xs
n <- 100
x <- sort(runif(n, 0,1))
y <- scale(10*sin(2*pi*x)*x+rnorm(n,0,1), scale = FALSE) # 0 mean 
# cubic spline 
cSmatrix <- cubSpline(x,seq(0,1,by=0.1))[,-1] # drop the intercept to ensure 0 mean
p <- ncol(cSmatrix)
# TODO: show some possible a-priori functions for different tau values 

```



## Generalised addtive models 

Till now we have use one covariate but in many situations we believe we will have several predictors influencing the variable of interest. 
For example, for the `airquality` data, we might want to model the ozone levels as a function of several predictors:

```{r}
data("airquality")
airquality <- na.omit(airquality) # not best practice... 
plot(airquality)
```



```{r}
tf.Xc <- function(x,knots, cmx=NULL){
  nk <- length(knots); n <- length(x)
  X <- tf.X(x=x,knots=knots)[,-nk] # remove one column
  D <- diff(diag(nk), differences  = 2)[,-nk]
  # cmx is the column means 
  # needed to evaluate function at new locations 
  if(is.null(cmx)) cmx <- colMeans(X)
  X <- sweep(X, 2, cmx)
  list(X=X,D=D,cmx=cmx)
}
dim(tf.X(mcycle$times, knots = seq(0,60,length.out=15)))
dim(tf.Xc(mcycle$times, knots = seq(0,60,length.out=15))[["X"]])
mean(tf.X(mcycle$times, knots = seq(0,60,length.out=15)) %*% rnorm(15,1,2))
mean(tf.Xc(mcycle$times, knots = seq(0,60,length.out=15))[["X"]] %*% rnorm(14,1,2))
```


Let's now see how the ozone levels change as a function of solar radiation and wind. First we construct the design matrix for the whole model: 

```{r}
data("airquality")
airquality <- na.omit(airquality)
plot(airquality[,c("Solar.R","Wind","Ozone")])
# create basis functions and D matrices for each predictor 
Msolar <- tf.Xc(x = airquality$Solar.R, knots = seq(min(airquality$Solar.R), max(airquality$Solar.R), length.out = 15))
Mwind <- tf.Xc(x = airquality$Wind, knots = seq(min(airquality$Wind), max(airquality$Wind), length.out = 12))
# number of coefficients for each predictor 
p1 <- ncol(Msolar$X); p2 <- ncol(Mwind$X)
# the overall design matrix 
X <- cbind(rep(1,nrow(airquality)), Msolar$X, Mwind$X)
# create the P1 and P2 matrices - they are mostly 0s 
Psolar <- matrix(0,ncol=1+p1+p2, nrow = 1+p1+p2); Psolar[2:(1+p1),2:(1+p1)] <-  crossprod(Msolar$D) 
Pwind <- matrix(0,ncol=1+p1+p2, nrow = 1+p1+p2); Pwind[(2+p1):(p1+p2+1),(2+p1):(p1+p2+1)] <- crossprod(Mwind$D) 
# take arbitrary lambdas 
sp1 <- 6; sp2 <- 4
# TODO: fit a first GAM and show the effect of each covariate
# TODO: check the mean of each f_j
# TODO: use the data augmentation approach to estimate the model: do the two model estimation approaches match? (some initial help is given)
# construct the B matrix 

```

Notice that for given $\lambda_1$ and $\lambda_2$ the model is estimated in one unique solution, avoiding backfitting. 

### mgcv 

Luckily for us we do not need to construct the whole model by hand but we can use packages which are optimized and well tested. The `mgcv` package, developed by [Simon Wood](https://www.maths.ed.ac.uk/~swood34/) is *the* R package to model generalised additive models, although later I list a number of other packages which are also very relevant. Let's see `mgcv` in practice:  

```{r class.source = 'fold-show'}
suppressPackageStartupMessages(library(mgcv))
fit_aq <- gam(Ozone~s(Solar.R)+s(Wind), data = airquality) # s() to obtain smooth terms 
# similar in structure to a summary.lm 
summary(fit_aq)
par(mfrow=c(1,2))
plot(fit_aq) # by default mgcv plots 95% confidence intervals 
```


```{r class.source = 'fold-show'}
## TODO: use cubic regression splines with respectively 15 and 12 knots to fit the model
## read the help file of ?s to find out which options to change
fit2_aq <- gam(Ozone~s(Solar.R,k=15, bs = "cr")+s(Wind, k=12, bs = "cr"), data = airquality, method = "REML")
```



Some small plots to comment on interesting aspects of the fit:

```{r}
# after all - we are actually fitting a linear model
# let's see what happens for the Wind variable 
X <- model.matrix(fit2_aq)[order(airquality$Wind),]
plot(fit2_aq, select = 2, se = FALSE, rug = FALSE, lwd = 2) 
lines(sort(airquality$Wind), 
      X[,grep("Wind",colnames(X))] %*% coef(fit2_aq)[grep("Wind",colnames(X))], 
      lty = 4, col = "red",lwd = 1.8)
# Let's look at the columns of the model matrix - 
plot(sort(airquality$Wind), 
      X[,grep("Wind",colnames(X))][,1], type="l", ylim = c(-.5,1.5),ylab="Basis") 
for(j in 2:length(grep("Wind",colnames(X)))) lines(sort(airquality$Wind),X[,grep("Wind",colnames(X))][,j],col=j)
rm(X,j)
```

### Some useful basis 

We now explore some useful basis functions available in GAM which allow for the estimation of fairly complex models in the same framework of penalized regression used in GAM smoothing: we start with an example of varying coefficient models (taken from the excellent book *Bayesian and Frequentist Regression Methods* by Jon Wakefield). 

```{r}
data(ethanol, package = "SemiPar")
ethanol <- ethanol[order(ethanol$E),] 
# help(ethanol)
with(ethanol, plot(NOx,C, pch = 16))
par(mfrow=c(2,2), pch = 16)
with(ethanol[ 1:22,], plot(NOx ~ C)); with(ethanol[ 1:22,], abline(lm(NOx ~ C)))
with(ethanol[23:44,], plot(NOx ~ C)); with(ethanol[23:44,], abline(lm(NOx ~ C)))
with(ethanol[45:66,], plot(NOx ~ C)); with(ethanol[45:66,], abline(lm(NOx ~ C)))
with(ethanol[67:88,], plot(NOx ~ C)); with(ethanol[67:88,], abline(lm(NOx ~ C)))
# effect of NOx on C is different for different levels of E 
```


```{r}
fit_eth <- gam(NOx~s(E,bs="cr")+s(E,by=C,bs="cr"), data = ethanol)
summary(fit_eth) 
# effective degrees of freedom of 6.4 and 4.7 on the intercept and slope smooths
par(mfrow=c(1,2))
plot(fit_eth, scale = 0)
# how are values fitted values  
predict(fit_eth, newdata = data.frame(E=1.2,C=c(10,16)), type = "terms")
predict(fit_eth, newdata = data.frame(E=1.2,C=c(10,16)))
predict(fit_eth, newdata = data.frame(E=0.6,C=c(10,16)), type = "terms")
predict(fit_eth, newdata = data.frame(E=0.6,C=c(10,16)))
# notice we have the usual functions we expect for linear model 
```


Let's look at the basis function for each of the terms: 

```{r class.source = 'fold-show'}
# Let's look at the columns of the model matrix - 
X <- model.matrix(fit_eth)
#colnames(X)
# basis used to estimate the intercept
plot(ethanol$E, 
      X[,grep("E)[.]",colnames(X))][,1], type="l", ylim = c(-.5,1.5),ylab="Basis")
title(main="Basis for the varying intercept")
for(j in 2:length(grep("E)[.]",colnames(X)))) lines(ethanol$E,X[,grep("E)[.]",colnames(X))][,j],col=j)
# basis for the slope 
plot(ethanol$E, 
      X[,grep("E)[:]",colnames(X))][,1], type="l", ylim = c(-2.5,4.5),ylab="Basis")
title(main="Basis for the varying slope")
for(j in 2:length(grep("E)[:]",colnames(X)))) lines(ethanol$E,X[,grep("E)[:]",colnames(X))][,j],col=j)
# something looks off 
plot(ethanol$E, 
      X[,grep("E)[:]",colnames(X))][,1]/ethanol$C, type="l", ylim = c(-.5,1.5),ylab="Basis")
title(main="Basis for the varying slope - processed")
for(j in 2:length(grep("E)[:]",colnames(X)))) lines(ethanol$E,X[,grep("E)[:]",colnames(X))][,j]/ethanol$C,col=j)
# back to normal - the actual basis is obtained by multiplying the original basis and the _by_ predictor 
rm(X,j)
```

In `mgcv` several types of predictors can be included, an overview can be seen in `?smooth.terms`. 


### The G of GAMs

Until now we have analysed data assuming a Gaussian distribution: this is hardly a general assumption, real data come in all forms, for example they might be discrete and/or skewed. `mgcv` allows for different distributions to be estimated: this can be specified via the `family` option. Let's look for example at the precipitation data from the German climate data: precipitation is by definition skewed and can only be positive, we could employ a Gamma distribution. Looking at the data though it is clear that something radically change in the data recording procedure around 1878: we only focus on the record after that year: 

```{r}
with(climDat, plot(year+(month-1)/12, Precip, pch = 16))
abline(v=1878, col = 2)
```

(don't forget that careful checks on the data are still a key element even when you can use advanced modelling techniques). 


```{r}
subclim <- climDat[climDat$year > 1878,] 
par(mfrow=c(1,2))
with(subclim, plot(year+(month-1)/12, Precip, pch = 16))
with(subclim, plot(month, Precip, pch = 16)) #  strong seasonal component 
# let's fit a GAM
fit_precip <- gam(Precip ~ s(year, k=20)+s(month, k = 12), family = "Gamma", data = subclim)
summary(fit_precip)
plot(fit_precip) # the term plots 
```

We see that the seasonal variation dominates the signal, a small trend in time is visible, but it is not a very noticeable signal (compare it to the one we can obtain for the temperature data, which can be assumed to be normally distributed). 

We have included month as if it was a continuous variable, but in reality we only have 12 possible values and the variable is not really continuous, but rather a cyclical, since 1 comes after 12. We can enforce the basis used to model a smooth term to take the same value in the last and first knot, and if we also enforce that the first and last parameter for the smooth term are equal, we obtain that the value at the first and last covariate value are the same: this can avoid awkward large discontinuities where we would actually want the function to be continuous. This can be obtained with cyclic splines, and using the `bs="cc"` option in `s`: 

```{r}
par(mfrow=c(1,2))
# let's fit a GAM
fit2_precip <- gam(Precip ~ s(year, k=20)+s(month, bs = "cc", k = 12), 
                   family = "Gamma", data = subclim)
# one less parameter to estimate
#dim(model.matrix(fit_precip)); dim(model.matrix(fit2_precip))
summary(fit2_precip)
plot(fit2_precip) 
```

The fit is not radically different. Let's compare the first 36 fitted values: 

```{r}
plot(predict(fit_precip, terms = "s(month)")[1:36], type = "l")
lines(predict(fit2_precip, terms = "s(month)")[1:36], type = "l", col = 2)
predict(fit_precip, terms = "s(month)")[c(12,13,24,25)]
predict(fit2_precip, terms = "s(month)")[c(12,13,24,25)]
```

We notice that December and January are forced to have the same value. For this application this might be somewhat restrictive, but when data is available at a finer scale (minute of the day, or angles around a circle) there could be some benefit in enforcing the end and the beginning of the record to have matching values. 

From the plots and the commands used above we notice that `mgcv::gam` operates in a way similar to `glm`: the actual estimation is carried out in the link-transformed space and the inverse link needs to be employed when we wish to retrieve estimates of the distribution's expected value:

```{r}
1/predict(fit_precip)[1:10]
fitted(fit_precip)[1:10]
```

A list of distributions available for `mgcv` (which are well beyond the exponential family) can be retrieved at `?family.mgcv`. Some of these are indicated as location-scale: for these distributions we allow parameters other than the location to also depend on the predictor, thus constructing generalized additive models for location, scale and shape (GAMLSS). 


## Not flexible enough - beyond the mean 

(I stole the title of the subsection from Thomas Kneib's [excellent paper](https://journals.sagepub.com/doi/10.1177/1471082X13494159)). 

Sometimes we find that it is not only the mean of our data that varies, but other properties at well. In the precipitation record we just analysed for example in the summer months we observe higher means and higher variability. Since the Gamma is already modeled taking $Var(Y) = \phi \mu$ we might think that we would capture this feature of the data in out model, but we can actually notice that the coefficient of variation is lower in the summer months. The increase in precipitation comes with a smaller increase in variation that the one seen in the winter months, while no clear signal is noticeable in the year to year signal: 

```{r}
par(mfrow=c(1,2))
plot(tapply(subclim$Precip,subclim$month,function(x) log(sd(x) / mean(x))), type="l", ylab = "CV")
plot(tapply(subclim$Precip,subclim$year,function(x) log(sd(x) / mean(x))), type="l", ylab = "CV")
```

We can then ask `mgcv` to also model the *scale* of the distribution: 

```{r}
fitls_precip <- gam(list(
                         Precip ~ s(year, k=20)+s(month, k = 12), # a model for the mean 
                         ~ s(year, k=20)+s(month, k = 12)), # a model for the scale
                    family = gammals, data = subclim, method = "REML", optimizer = "efs")
summary(fitls_precip)
plot(fitls_precip, page = 1)
```

(notice that `gammals` uses the identity link for the log-location, rather than the inverse used by `Gamma`)

We see that the scale appears to be changing with month, but no effect is found for `year`, we can change our model accordingly: 

```{r}
fitls_precip <- gam(list(
                         Precip ~ s(year, k=20)+s(month, k = 12), # a model for the mean 
                         ~ s(month, k = 12)), # a model for the scale
                    family = gammals, data = subclim, method = "REML", optimizer = "efs")
summary(fitls_precip)
plot(fitls_precip, page = 1)
```

We can plot the fitted location ans scale (only the first 40 years to avoid clogging the figure): 

```{r}
# TODO: read the help file of gammals and plot the fitted location and scale function 

```

(Notice that the scale function is on a non-intuitive scale, i.e. the log of the actual scale)


### `gamlss` 

We briefly show how to use the `gamlss` package to fit a model for location, scale and shape. We start with the same model for the precipitation data based on the location and scale of a Gamma distribution: 


```{r}
library(gamlss)
fitgamlss_precip <- gamlss(Precip ~ pb(year)+pb(month), # a model for the mean
                           sigma.formula = ~ pb(month), # a model for the scale
                     family = GA, data = subclim[,c("month","year","Precip")])
summary(fitgamlss_precip)
plot(fitgamlss_precip) # a different default for what to plot
# similar effects to what mgcv found
# to plot the terms we need to use predict/predcitAll 
par(mfrow=c(2,2))
pout <- predictAll(fitgamlss_precip, type = "terms")
plot(subclim$year, pout$mu[,1], type="l",ylab="mu(year)")
plot(seq(1,12), pout$mu[1:12,2], type="l",ylab="mu(month)")
plot(seq(1,12), pout$sigma[1:12,1], type="l",ylab="sigma(month)")
```


```{r}
fitgamlss2_precip <- gamlss(Precip ~ pb(year)+pb(month), # a model for the mean
                            sigma.formula = ~ pb(month), # a model for the scale
                            nu.formula = ~ 1,            # no model for shape yet
                     family = BCCG(), data = subclim[,c("month","year","Precip","TMean")])
summary(fitgamlss2_precip)
plot(fitgamlss2_precip) #  residuals a bit better behaved 
# to plot the terms we need to use predict/predcitAll 
par(mfrow=c(2,2))
pout <- predictAll(fitgamlss2_precip, type = "terms")
plot(subclim$year, pout$mu[,1], type="l",ylab="mu(year)")
plot(seq(1,12), pout$mu[1:12,2], type="l",ylab="mu(month)")
plot(seq(1,12), pout$sigma[1:12,1], type="l",ylab="sigma(month)")
```

We see that the additional parameter captures the additional skewness. We could indeed assess whether the shape of the distribution also changes a function of some covariate. 


```{r}
# TODO: add a smooth term term for the shape parameter to change as a function of year
fitgamlss3_precip <- NA
#TODO: plot the the different terms for the different predictors 
```


Overall GAM(LSS) offer a great deal of flexibility in our modelling, this addiotional flexibility requires careful checks of the modelling assumptions which are not discussed here but should be part of a real data analysis. These are based on the same principles as in GLM 



## Other approaches/material

As you might have noticed `mgcv` is very `base-R` oriented, but packages exist to make their manipulation more tidyverse compliant: `broom` has tidyers for `gam` objects and Gavin Simpson develops and maintains [`gratia`](https://gavinsimpson.github.io/gratia/), which provides functions to plot outputs of `gam` objects via ggplot (his [blog](https://gavinsimpson.github.io/gratia/) and papers are an excellent source of material on GAMs, mostly skewed on mgcv). 


Rigby and Stasinopoulos (2005, 10.1111/j.1467-9876.2005.00510.x) introduced the concept of GAMLSS and develop and maintain the original `gamlss` package and [project](https://www.gamlss.com/). The package implements almost 100 distributions which can be employed as distributions for the response variable. A number of smoothers are implemented, and several strategies can be implemented for smoothing parameter selection. 

The Rigby and Stasinopoulos approach is mostly frequentist, while a fully Bayesian approach to additive models for location scale and shape is taken by Umlauf et al. (2021, 10.18637/jss.v100.i04) in their `bamlss` package, which employs `mgcv` functions to construct the basis and penalty matrix and can fit all families implemented in `gamlss` and additional families included in `bamlss`. One of the interesting distributions implemented in `bamlss` is the Asymmetric Laplace distribution which allows for the smooth estimation of quantile regression. 


Finally [`brms`](https://paul-buerkner.github.io/brms/) is a very general purpose package for Bayesian regression modelling via Stan - the package has a helpful [vignette](https://paul-buerkner.github.io/brms/articles/brms_distreg.html) introducing how `brms` implements distributional regression models. The `brms` implementation of smoothing is fully Bayesian and exploits the mixed model representation of penalized regression (TJ Mahr has a very clear dissection of this in his [blog post](https://www.tjmahr.com/random-effects-penalized-splines-same-thing/)). 


The `VGAM` package, developed and maintained by T. Yee, further allows for multi-variate responses, a large number of smoothers and distributions. 


